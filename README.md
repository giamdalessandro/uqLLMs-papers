## Uncertainty
<table> 
    <thead> <tr><th>Title</th> <th>Authors</th> <th>Year</th> <th>TLDR</th> <th>Links</th> </tr> </thead> 
    <tbody> 
        <tr><td>
            <em>Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs</em></td> <td>Vazhentsev et al.</td> <td>2025</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2505.20045">PDF</a> · <a href="#">Code</a> </td> </tr> 
        <tr><td>
            <em>A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions</em></td> <td>Shorinwa et al.</td> <td>2024</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2412.05563">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
        <tr><td>
            <em>Benchmarking LLMs via Uncertainty Quantification</em></td> <td>Ye et al.</td> <td>2024</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2401.12794">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
        <tr><td> 
            <em>Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach</em></td> <td>Liu et al.</td> <td>2024</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2404.15993">PDF</a> · <a href="#">Code</a> 
        </td></tr>
        <tr><td>
            <em>Conformal Prediction with Large Language Models for Multi-Choice Question Answering</em></td> <td>Kumar et al.</td> <td>2023</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2305.18404">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
    </tbody> 
</table>


## Hallucinations
<table> 
    <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>TLDR</th> <th>Links</th> </tr> </thead> 
    <tbody> 
        <tr><td>
            <em>Why Language Models Hallucinate</em></td> <td>Kalai et al.</td> <td>2025</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/pdf/2509.04664">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
    </tbody>
    <tbody> 
        <tr><td>
            <em>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</em></td> <td>Chuang et al.</td> <td>2024</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> <a href="https://arxiv.org/abs/2407.07071">PDF</a> · <a href="https://github.com/voidism/Lookback-Lens">Code</a> 
        </td></tr>
    </tbody> 
</table>

## MCQA
<table> 
    <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>TLDR</th> <th>Links</th> </tr> </thead> 
    <tbody> 
        <tr><td>
            <em>Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering</em></td> <td>Molfese et al.</td> <td>2025</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> 
            <a href="https://arxiv.org/abs/2503.14996">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
    </tbody>
</table>


## Bonus

<!-- ### Attention maps

#### Parsing
<table> 
    <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>TLDR</th> <th>Links</th> </tr> </thead>
    
</table> -->


#### Attention Sinks
<table> 
    <thead> <tr> <th>Title</th> <th>Authors</th> <th>Year</th> <th>TLDR</th> <th>Links</th> </tr> </thead>
    <tbody> 
        <tr><td>
            <em>What are you sinking? A geometric approach on attention sink</em></td> <td>Ruscio et al.</td> <td>2025</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> <a href="https://arxiv.org/pdf/2508.02546">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
        <tr><td>
            <em>Why do LLMs attend to the first token?</em></td> <td>Barbero et al.</td> <td>2025</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> <a href="https://arxiv.org/pdf/2504.02732">PDF</a> · <a href="#">Code</a> 
        </td></tr> 
        <!-- <tr><td>
            <em>Efficient Streaming Language Models with Attention Sinks</em></td> <td>Xiao et al.</td> <td>2024</td> <td> <details> <summary>Show summary</summary> A few lines describing the core ideas. </details> </td> <td> <a href="https://arxiv.org/pdf/2309.17453">PDF</a> · <a href="https://github.com/mit-han-lab/streaming-llm">Code</a>
        </td></tr>  -->
    </tbody> 
</table>