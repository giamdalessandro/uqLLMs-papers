# uqLLMs-papers
Paper on Uncertainty Quantification for Large Language models.

## MCQA
<details>
<summary><strong>Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering</strong> — (Molfese et al, 2025)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

## Uncertainty
<details>
<summary><strong>Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs</strong> — (Vazhentsev et al, 2025)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

<details>
<summary><strong>A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions</strong> — (Shorinwa et al, 2024)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

<details>
<summary><strong>Benchmarking LLMs via Uncertainty Quantification</strong> — (Ye et al, 2024)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

<details>
<summary><strong>Uncertainty Estimation and Quantification for LLMs: A Simple  Supervised Approach</strong> — (Liu et al., 2024)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

<details>
<summary><strong>Conformal Prediction with Large Language Models for Multi-Choice Question Answering</strong> — (Kumar et al, 2023)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>


## Hallucinations
<details>
<summary><strong>Why Language Models Hallucinate</strong> — (Kalai et al, 2025)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>


## Attention Sinks
<details>
<summary><strong>What are you sinking? A geometric approach on attention sink</strong> — (Ruscio et al, 2025)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>

<details>
<summary><strong>Why do LLMs attend to the first token?</strong> — (Barbero et al., 2025)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>  

<details>
<summary><strong>Efficient Streaming Language Models with Attention Sinks</strong> — (Xiao et al, 2024)</summary>

**Summary:**  
A few lines describing the core ideas.
- [PDF](#)  
- [Code](#)
</details>  
